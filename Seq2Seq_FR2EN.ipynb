{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/homebrew/anaconda3/envs/snp/lib/python3.11/site-packages (2.0.1)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/anaconda3/envs/snp/lib/python3.11/site-packages (from torch) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in /opt/homebrew/anaconda3/envs/snp/lib/python3.11/site-packages (from torch) (4.6.3)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/anaconda3/envs/snp/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/anaconda3/envs/snp/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/anaconda3/envs/snp/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/anaconda3/envs/snp/lib/python3.11/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/anaconda3/envs/snp/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary libraries \n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "import time \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "#Check for CUDA availability and set the device to GPU if available, else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Ignore duplicate warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>French</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Va !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Marche.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>En route !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Bouge !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  English      French\n",
       "0     Go.        Va !\n",
       "1     Go.     Marche.\n",
       "2     Go.  En route !\n",
       "3     Go.     Bouge !\n",
       "4     Hi.     Salut !"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Define file path and load the data into a DataFrame\n",
    "data_path = '/opt/homebrew/anaconda3/NLP/Teachback code/Data/fra-eng/fra.txt'\n",
    "df = pd.read_csv(data_path,delimiter='\\t',header=None,usecols=[0,1])\n",
    "df.columns = ['English','French']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Read 227815 sentence pairs\n",
      "Trimmed to 17917 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 5699\n",
      "eng 3703\n",
      "['j essaie de me rappeler', 'i m trying to remember']\n"
     ]
    }
   ],
   "source": [
    "#Define SOS_token and EOS_token constants as 0 and 1 \n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "#Language class to store word-to-index and index-to-word mappings for a language\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        ''' Constructor for the Lang class.\n",
    "        \n",
    "        Input Parameters: name (str) - The name of the language.\n",
    "        \n",
    "        Output Parameters: None '''\n",
    "        self.name = name\n",
    "        self.word2index = {}  #Mapping from word to index\n",
    "        self.word2count = {}  #Count of occurrences of each word\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}  #Mapping from index to word\n",
    "        self.n_words = 2  #Total number of words in the language, initialized with SOS and EOS tokens\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        ''' Add a sentence to the language object, updating the word-to-index and index-to-word mappings.\n",
    "       \n",
    "        Input Parameters: sentence (str) - The input sentence.\n",
    "        \n",
    "        Output Parameters: None '''\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        ''' Add a word to the language object, updating the word-to-index and index-to-word mappings.\n",
    "        \n",
    "        Input Parameters: word (str) - The input word.\n",
    "        \n",
    "        Output Parameters: None '''\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "#Function to convert a Unicode string to plain ASCII\n",
    "def unicodeToAscii(s):\n",
    "    ''' Convert a Unicode string to plain ASCII by removing diacritics.\n",
    "    \n",
    "    Input Parameters: s (str) - The input Unicode string.\n",
    "    \n",
    "    Output Parameters: str - The resulting plain ASCII string. '''\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "#Function to normalize a string by converting it to  lowercase, triming, and removing the non-letter characters\n",
    "def normalizeString(s):\n",
    "    ''' Normalize a string by converting it to lowercase, trimming, and removing non-letter characters.\n",
    "    \n",
    "    Input Parameters: s (str) - The input string.\n",
    "    \n",
    "    Output Parameters: str - The normalized string. '''\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)  #Add spaces before punctuation marks\n",
    "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)  #Remove non-letter characters\n",
    "    return s.strip()\n",
    "\n",
    "#Function to read and preprocess the language pairs from a DataFrame\n",
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    ''' Read language pairs from a DataFrame, normalize them, and create Lang class instances.\n",
    "    \n",
    "    Input Parameters: lang1 (str) - Name of the first language.\n",
    "                      lang2 (str) - Name of the second language.\n",
    "                      reverse (bool) - If True, reverse the order of language pairs.\n",
    "\n",
    "    Output Parameters: input_lang (Lang) - Language object for the first language.\n",
    "                       output_lang (Lang) - Language object for the second language.\n",
    "                       pairs (list) - List of language pairs, where each pair is a list containing two sentences. '''\n",
    "    print(\"Reading data...\")\n",
    "    #Get English and French sentences from the DataFrame\n",
    "    eng_sentences = df['English'].tolist()\n",
    "    fra_sentences = df['French'].tolist()\n",
    "    #Combine sentences into pairs and normalize the sentences\n",
    "    pairs = [[normalizeString(eng), normalizeString(fra)] for eng, fra in zip(eng_sentences, fra_sentences)]\n",
    "    #Reverse pairs if required and create Lang class instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "#Function to filter a language pair based on their lengths and prefixes\n",
    "def filterPair(p):\n",
    "    ''' Filter a language pair based on the lengths of both sentences and the prefixes of the second sentence.\n",
    "    \n",
    "    Input Parameters: p (list) - A language pair, where each pair is a list containing two sentences.\n",
    "\n",
    "    Output Parameters: bool - True if the pair satisfies the filtering conditions, False otherwise. '''\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "           len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "           p[1].startswith(eng_prefixes)\n",
    "\n",
    "#Function to filter a list of language pairs\n",
    "def filterPairs(pairs):\n",
    "    ''' Filter a list of language pairs using the filterPair function.\n",
    "    \n",
    "    Input Parameters: pairs (list) - List of language pairs, where each pair is a list containing two sentences.\n",
    "\n",
    "    Output Parameters: list - List of filtered language pairs. '''\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "#Define a maximum sentence length and common english sentence prefixes\n",
    "MAX_LENGTH = 10\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "#Function to prepare the data for training\n",
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    ''' Prepare the data for training by reading language pairs, filtering them, and creating language objects.\n",
    "    \n",
    "    Input Parameters: lang1 (str) - Name of the first language.\n",
    "                      lang2 (str) - Name of the second language.\n",
    "                      reverse (bool) - If True, reverse the order of language pairs.\n",
    "   \n",
    "     Output Parameters: input_lang (Lang) - Language object for the first language.\n",
    "                       output_lang (Lang) - Language object for the second language.\n",
    "                       pairs (list) - List of filtered language pairs. '''\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "#Function call to prepare the data \n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
    "#Printing sample pairs \n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EncoderRNN class to define the Encoder part of the Seq2Seq model\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        ''' Constructor for the EncoderRNN class.\n",
    "\n",
    "        Input Parameters: input_size (int) - The size of the input vocabulary.\n",
    "                          hidden_size (int) - The size of the hidden state of the GRU.\n",
    "                          dropout_p (float) - Dropout probability for the dropout layer.\n",
    "\n",
    "        Output Parameters: None '''\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        ''' Forward pass of the EncoderRNN.\n",
    "\n",
    "        Input Parameters: input (torch.Tensor) - The input tensor representing the input sentences.\n",
    "\n",
    "        Output Parameters: output (torch.Tensor) - The output tensor from the GRU layer.\n",
    "                           hidden (torch.Tensor) - The hidden state of the GRU. '''\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, hidden = self.gru(embedded)\n",
    "        return output, hidden\n",
    "\n",
    "#DecoderRNN class to define the Decoder part of the Seq2Seq model\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        ''' Constructor for the DecoderRNN class.\n",
    "\n",
    "        Input Parameters: hidden_size (int) - The size of the hidden state of the GRU.\n",
    "                          output_size (int) - The size of the output vocabulary.\n",
    "\n",
    "        Output Parameters: None '''\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        ''' Forward pass of the DecoderRNN.\n",
    "\n",
    "        Input Parameters: encoder_outputs (torch.Tensor) - The output tensor from the EncoderRNN.\n",
    "                          encoder_hidden (torch.Tensor) - The hidden state of the EncoderRNN.\n",
    "                          target_tensor (torch.Tensor) - The target tensor representing the target sentences. (Optional)\n",
    "\n",
    "        Output Parameters: decoder_outputs (torch.Tensor) - The output tensor from the DecoderRNN.\n",
    "                           decoder_hidden (torch.Tensor) - The hidden state of the DecoderRNN.\n",
    "                           None - As attention is not used in this model, None is returned for consistency in the training loop. '''\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            if target_tensor is not None:\n",
    "                #Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1) #Teacher forcing\n",
    "            else:\n",
    "                #Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  #detach from history as input\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        return decoder_outputs, decoder_hidden, None  #We return `None` for consistency in the training loop\n",
    "\n",
    "    def forward_step(self, input, hidden):\n",
    "        ''' Forward step of the DecoderRNN.\n",
    "\n",
    "        Input Parameters: input (torch.Tensor) - The input tensor representing the input sentence.\n",
    "                          hidden (torch.Tensor) - The hidden state of the DecoderRNN.\n",
    "\n",
    "        Output Parameters: output (torch.Tensor) - The output tensor from the DecoderRNN.\n",
    "                           hidden (torch.Tensor) - The hidden state of the DecoderRNN. '''\n",
    "        output = self.embedding(input)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.out(output)\n",
    "        return output, hidden\n",
    "\n",
    "#BahdanauAttention class to define the Attention mechanism for the DecoderRNN\n",
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        ''' Constructor for the BahdanauAttention class.\n",
    "\n",
    "        Input Parameters: hidden_size (int) - The size of the hidden state of the GRU.\n",
    "\n",
    "        Output Parameters: None '''\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Va = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        ''' Forward pass of the BahdanauAttention.\n",
    "\n",
    "        Input Parameters: query (torch.Tensor) - The query tensor representing the hidden state of the DecoderRNN.\n",
    "                          keys (torch.Tensor) - The keys tensor representing the output from the EncoderRNN.\n",
    "\n",
    "        Output Parameters: context (torch.Tensor) - The context tensor, the weighted sum of encoder outputs based on attention.\n",
    "                           weights (torch.Tensor) - The attention weights indicating the importance of each encoder output. '''\n",
    "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        context = torch.bmm(weights, keys)\n",
    "        return context, weights\n",
    "\n",
    "#AttnDecoderRNN class to define the Decoder with Attention mechanism\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
    "        ''' Constructor for the AttnDecoderRNN class.\n",
    "\n",
    "        Input Parameters: hidden_size (int) - The size of the hidden state of the GRU.\n",
    "                          output_size (int) - The size of the output vocabulary.\n",
    "                          dropout_p (float) - Dropout probability for the dropout layer.\n",
    "\n",
    "        Output Parameters: None '''\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.attention = BahdanauAttention(hidden_size)\n",
    "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        ''' Forward pass of the AttnDecoderRNN.\n",
    "\n",
    "        Input Parameters: encoder_outputs (torch.Tensor) - The output tensor from the EncoderRNN.\n",
    "                          encoder_hidden (torch.Tensor) - The hidden state of the EncoderRNN.\n",
    "                          target_tensor (torch.Tensor) - The target tensor representing the target sentences. (Optional) \n",
    "\n",
    "        Output Parameters: decoder_outputs (torch.Tensor) - The output tensor from the AttnDecoderRNN.\n",
    "                           decoder_hidden (torch.Tensor) - The hidden state of the AttnDecoderRNN.\n",
    "                           attentions (torch.Tensor) - The attention weights indicating the importance of each encoder output for each timestep. '''\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "        attentions = []\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden, attn_weights = self.forward_step(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            attentions.append(attn_weights)\n",
    "            if target_tensor is not None:\n",
    "                #Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1) #Teacher forcing\n",
    "            else:\n",
    "                #Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  #detach from history as input\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        attentions = torch.cat(attentions, dim=1)\n",
    "        return decoder_outputs, decoder_hidden, attentions\n",
    "\n",
    "    def forward_step(self, input, hidden, encoder_outputs):\n",
    "        ''' Forward step of the AttnDecoderRNN.\n",
    "\n",
    "        Input Parameters: input (torch.Tensor) - The input tensor representing the input sentence.\n",
    "                          hidden (torch.Tensor) - The hidden state of the DecoderRNN.\n",
    "                          encoder_outputs (torch.Tensor) - The output tensor from the EncoderRNN.\n",
    "                          \n",
    "        Output Parameters: output (torch.Tensor) - The output tensor from the AttnDecoderRNN.\n",
    "                           hidden (torch.Tensor) - The hidden state of the AttnDecoderRNN.\n",
    "                           attn_weights (torch.Tensor) - The attention weights indicating the importance of each encoder output. '''\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        query = hidden.permute(1, 0, 2)\n",
    "        context, attn_weights = self.attention(query, encoder_outputs)\n",
    "        input_gru = torch.cat((embedded, context), dim=2)\n",
    "        output, hidden = self.gru(input_gru, hidden)\n",
    "        output = self.out(output)\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to convert a sentence to a list of indexes using a language class object\n",
    "def indexesFromSentence(lang, sentence):\n",
    "    ''' Convert a sentence to a list of indexes using the word-to-index mapping of the given language object.\n",
    "\n",
    "    Input Parameters: lang (Lang) - The language object containing the word-to-index mapping.\n",
    "                      sentence (str) - The input sentence.\n",
    "\n",
    "    Output Parameters: list - A list of indexes representing the input sentence. '''\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "#Function to convert a sentence to a PyTorch tensor using a language object\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    ''' Convert a sentence to a PyTorch tensor using the word-to-index mapping of the given language object.\n",
    "    \n",
    "    Input Parameters: lang (Lang) - The language object containing the word-to-index mapping.\n",
    "                      sentence (str) - The input sentence.\n",
    "\n",
    "    Output Parameters: torch.Tensor - A PyTorch tensor representing the input sentence. '''\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
    "\n",
    "#Function to create PyTorch tensors from a language pair\n",
    "def tensorsFromPair(pair):\n",
    "    ''' Create PyTorch tensors from a language pair.\n",
    "\n",
    "    Input Parameters: pair (tuple) - A tuple containing two sentences, the source sentence and the target sentence.\n",
    "\n",
    "    Output Parameters: tuple - A tuple containing two PyTorch tensors, one for the source sentence and one for the target sentence. '''\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "#Function to create a DataLoader for the training data\n",
    "def get_dataloader(batch_size):\n",
    "    ''' Create a DataLoader for the training data.\n",
    "\n",
    "    Input Parameters: batch_size (int) - The batch size for the DataLoader.\n",
    "\n",
    "    Output Parameters: input_lang (Lang) - The language object for the source language.\n",
    "                       output_lang (Lang) - The language object for the target language.\n",
    "                       train_dataloader (DataLoader) - The DataLoader for the training data. '''\n",
    "    input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
    "    n = len(pairs)\n",
    "    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "\n",
    "    for idx, (inp, tgt) in enumerate(pairs):\n",
    "        inp_ids = indexesFromSentence(input_lang, inp)\n",
    "        tgt_ids = indexesFromSentence(output_lang, tgt)\n",
    "        inp_ids.append(EOS_token)\n",
    "        tgt_ids.append(EOS_token)\n",
    "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
    "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
    "    #Convert the numpy arrays to PyTorch tensors\n",
    "    input_ids_tensor = torch.LongTensor(input_ids).to(device)\n",
    "    target_ids_tensor = torch.LongTensor(target_ids).to(device)\n",
    "    #Create the TensorDataset\n",
    "    train_data = TensorDataset(input_ids_tensor, target_ids_tensor)\n",
    "    #Use RandomSampler for shuffling the data during training\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    #Create the DataLoader\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "    return input_lang, output_lang, train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to train one epoch\n",
    "def train_epoch(dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "    ''' Train one epoch of the Seq2Seq model.\n",
    "    \n",
    "    Input Parameters: dataloader (DataLoader) - The DataLoader containing the training data.\n",
    "                      encoder (EncoderRNN) - The EncoderRNN model.\n",
    "                      decoder (AttnDecoderRNN) - The AttnDecoderRNN model.\n",
    "                      encoder_optimizer (torch.optim) - The optimizer for the EncoderRNN.\n",
    "                      decoder_optimizer (torch.optim) - The optimizer for the AttnDecoderRNN.\n",
    "                      criterion (torch.nn) - The loss function.\n",
    "\n",
    "    Output Parameters: float - The average loss over the entire epoch. '''\n",
    "    total_loss = 0\n",
    "    #Loop through the DataLoader\n",
    "    for data in dataloader:\n",
    "        input_tensor, target_tensor = data\n",
    "        #Zero the gradients for both the optimizers\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        #Get the encoder outputs and hidden state\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        #Get the decoder outputs, decoder hidden state, and attention weights\n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "        #Calculate the loss\n",
    "        loss = criterion(decoder_outputs.view(-1, decoder_outputs.size(-1)),target_tensor.view(-1))\n",
    "        loss.backward()\n",
    "        #Update the weights using the optimizers\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "        #Accumulate the loss for the entire epoch\n",
    "        total_loss += loss.item()\n",
    "    #Calculate and return the average loss over the entire epoch\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to convert time in seconds to a human-readable format in minutes and seconds\n",
    "def asMinutes(s):\n",
    "    ''' Convert time in seconds to a human-readable format in minutes and seconds.\n",
    "\n",
    "    Input Parameters: s (float) - The time in seconds.\n",
    "\n",
    "    Output Parameters: str - A formatted string representing time in minutes and seconds. '''\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "#Function to calculate the time remaining until completion, given the starting time and completion percentage\n",
    "def timeSince(since, percent):\n",
    "    ''' Calculate the time remaining until completion given the starting time and completion percentage.\n",
    "\n",
    "    Input Parameters: since (float) - The starting time in seconds.\n",
    "                      percent (float) - The completion percentage.\n",
    "\n",
    "    Output Parameters: str - A formatted string representing the time remaining until completion. '''\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to train the Seq2Seq model\n",
    "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,\n",
    "               print_every=100, plot_every=100):\n",
    "    ''' Train the Seq2Seq model for a specified number of epochs.\n",
    "\n",
    "    Input Parameters: train_dataloader (DataLoader) - The DataLoader containing the training data.\n",
    "                      encoder (EncoderRNN) - The EncoderRNN model.\n",
    "                      decoder (AttnDecoderRNN) - The AttnDecoderRNN model.\n",
    "                      n_epochs (int) - The number of epochs to train the model.\n",
    "                      learning_rate (float) - The learning rate for the optimizer. (Default: 0.001)\n",
    "                      print_every (int) - Frequency of printing training progress (in number of epochs). \n",
    "                      plot_every (int) - Frequency of plotting training progress (in number of epochs). \n",
    "                      \n",
    "    Output Parameters: None '''\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  #Reset every print_loss_total\n",
    "    plot_loss_total = 0  #Reset every plot_loss_total\n",
    "    #Create optimizer and loss function\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "    #Loop through the specified number of epochs\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        #Train one epoch\n",
    "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "        #Print progress every print_every epochs\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),epoch, epoch / n_epochs * 100, print_loss_avg))\n",
    "        #Plot progress every plot_every epochs\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "    #Plot the training loss curve\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.switch_backend('agg')\n",
    "#Function to plot the training loss curve\n",
    "def showPlot(points):\n",
    "    ''' Plot the training loss curve.\n",
    "\n",
    "    Input Parameters: points (list) - A list of training loss values at different points in the training.\n",
    "\n",
    "    Output Parameters: None '''\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2) #Set y-axis tick intervals\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points) #Plot the training loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Read 227815 sentence pairs\n",
      "Trimmed to 17917 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 5699\n",
      "eng 3703\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 128 #Set the hidden size of the RNN models\n",
    "batch_size = 32 #Set the batch size for the DataLoader\n",
    "#Get the language objects and training DataLoader using the get_dataloader function\n",
    "input_lang, output_lang, train_dataloader = get_dataloader(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1m 44s (- 26m 3s) (5 6%) 1.3848\n",
      "3m 36s (- 25m 16s) (10 12%) 0.5538\n",
      "5m 34s (- 24m 11s) (15 18%) 0.2923\n",
      "7m 22s (- 22m 8s) (20 25%) 0.1813\n",
      "9m 13s (- 20m 17s) (25 31%) 0.1298\n",
      "11m 10s (- 18m 37s) (30 37%) 0.1021\n",
      "12m 48s (- 16m 27s) (35 43%) 0.0853\n",
      "14m 26s (- 14m 26s) (40 50%) 0.0751\n",
      "16m 12s (- 12m 36s) (45 56%) 0.0678\n",
      "17m 53s (- 10m 43s) (50 62%) 0.0630\n",
      "19m 43s (- 8m 57s) (55 68%) 0.0592\n",
      "21m 23s (- 7m 7s) (60 75%) 0.0563\n",
      "23m 6s (- 5m 19s) (65 81%) 0.0543\n",
      "24m 45s (- 3m 32s) (70 87%) 0.0521\n",
      "26m 26s (- 1m 45s) (75 93%) 0.0506\n",
      "28m 14s (- 0m 0s) (80 100%) 0.0492\n"
     ]
    }
   ],
   "source": [
    "#Create an instance of the EncoderRNN with the specified input size and hidden size\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "#Create an instance of the AttnDecoderRNN with the specified hidden size and output size\n",
    "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
    "#Train the models using the train function for 80 epochs\n",
    "#The print_every and plot_every arguments are set to 5, meaning the training progress will be printed and plotted every 5 epochs\n",
    "train(train_dataloader, encoder, decoder, 80, print_every=5, plot_every=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to evaluate the trained Seq2Seq model\n",
    "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
    "    ''' Evaluate the input sentence using the trained encoder and decoder models.\n",
    "    \n",
    "    Input Parameters: encoder (EncoderRNN) - The trained EncoderRNN model.\n",
    "                      decoder (AttnDecoderRNN) - The trained AttnDecoderRNN model.\n",
    "                      sentence (str) - The input sentence to be evaluated.\n",
    "                      input_lang (Lang) - The language object for the source language.\n",
    "                      output_lang (Lang) - The language object for the target language.\n",
    "\n",
    "    Output Parameters: decoded_words (list) - A list of decoded words in the target language.\n",
    "                       decoder_attn (torch.Tensor) - The attention weights generated by the decoder. '''\n",
    "    #Disable gradient calculation since we are in evaluation mode\n",
    "    with torch.no_grad():\n",
    "        #Convert the input sentence to PyTorch tensor\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        #Get the encoder outputs and hidden state\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        #Get the decoder outputs, decoder hidden state, and attention weights\n",
    "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
    "        #Get the indices of the top probabilities from the decoder outputs\n",
    "        _, topi = decoder_outputs.topk(1)\n",
    "        decoded_ids = topi.squeeze()\n",
    "        #Convert the indices to words using the output language object\n",
    "        decoded_words = []\n",
    "        for idx in decoded_ids:\n",
    "            if idx.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            decoded_words.append(output_lang.index2word[idx.item()])\n",
    "    #Return the list of decoded words and the attention weights\n",
    "    return decoded_words, decoder_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    '''Evaluate and print random examples from the training data using the trained encoder and decoder models.\n",
    "    \n",
    "    Input Parameters: encoder (EncoderRNN) - The trained EncoderRNN model.\n",
    "                      decoder (AttnDecoderRNN) - The trained AttnDecoderRNN model.\n",
    "                      n (int) - The number of random examples to evaluate and print. (Default: 10)\n",
    "\n",
    "    Output Parameters: None '''\n",
    "    #Loop through n random examples\n",
    "    for i in range(n):\n",
    "        #Choose a random pair from the training data\n",
    "        pair = random.choice(pairs)\n",
    "        #Print the input and target sentences\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        #Evaluate the input sentence using the evaluate function to get the output words\n",
    "        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
    "        #Convert the output words to a sentence and print them\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> on ne reussira pas a arriver a l heure\n",
      "= we re not going to make it in time\n",
      "< we re not going to make it in time <EOS>\n",
      "\n",
      "> je ne suis pas facilement offensee\n",
      "= i m not easily offended\n",
      "< i m not easily offended <EOS>\n",
      "\n",
      "> je suis heureux de te voir ici\n",
      "= i am happy to see you here\n",
      "< i am happy to see you here <EOS>\n",
      "\n",
      "> tu n es pas tres amusant\n",
      "= you re not very funny\n",
      "< you re not very funny <EOS>\n",
      "\n",
      "> tu es tres observateur\n",
      "= you re very observant\n",
      "< you re very observant <EOS>\n",
      "\n",
      "> tu es desormais en securite\n",
      "= you re safe now\n",
      "< you re safe now <EOS>\n",
      "\n",
      "> on m a promue\n",
      "= i m being promoted\n",
      "< i m being promoted <EOS>\n",
      "\n",
      "> ce n est qu une enfant\n",
      "= she s only a child\n",
      "< she is a mere child <EOS>\n",
      "\n",
      "> tu vas y arriver apres tout\n",
      "= you re going to make it after all\n",
      "< you re going to make it after all <EOS>\n",
      "\n",
      "> vous lisez mes pensees\n",
      "= you re reading my mind\n",
      "< you re reading my mind <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Set the encoder and decoder models to evaluation mode\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "#Call the evaluateRandomly function to evaluate and print random examples\n",
    "#The encoder and decoder models are in evaluation mode, so they won't update their weights during evaluation\n",
    "#The evaluateRandomly function randomly selects examples from the training data and generates translations using the trained models\n",
    "evaluateRandomly(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to show the attention matrix as a heatmap\n",
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    ''' Show the attention matrix as a heatmap.\n",
    "\n",
    "    Input Parameters: input_sentence (str) - The input sentence in the source language.\n",
    "                      output_words (list) - A list of decoded words in the target language.\n",
    "                      attentions (torch.Tensor) - The attention weights generated by the decoder.\n",
    "                      \n",
    "    Output Parameters: None '''\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.cpu().numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "    #Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') + ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "    #Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    #Show the plot\n",
    "    plt.show()\n",
    "\n",
    "#Function to evaluate the input sentence, generate output, and show attention heatmap\n",
    "def evaluateAndShowAttention(input_sentence):\n",
    "    ''' Evaluate the input sentence using the trained encoder and decoder models,generate the output translation, and show the attention heatmap.\n",
    "\n",
    "    Input Parameters: input_sentence (str) - The input sentence in the source language.\n",
    "\n",
    "    Output Parameters: None '''\n",
    "    #Evaluate the input sentence using the trained encoder and decoder models\n",
    "    output_words, attentions = evaluate(encoder, decoder, input_sentence, input_lang, output_lang)\n",
    "    #Print the input sentence and the generated output translation\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    #Show the attention heatmap for the input sentence and output translation\n",
    "    #The attention matrix 'attentions' has shape (target_length, input_length),but we are interested in the attention from each target word to each input word.\n",
    "    #Therefore, we select the attentions for the target words in the range of the generated output translation.\n",
    "    showAttention(input_sentence, output_words, attentions[0, :len(output_words), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = il n est pas aussi grand que son pere\n",
      "output = he is not as tall as his father <EOS>\n",
      "input = je suis trop fatigue pour conduire\n",
      "output = i m too tired to drive <EOS>\n",
      "input = je suis desole si c est une question idiote\n",
      "output = i m sorry if this is a stupid question <EOS>\n",
      "input = je suis reellement fiere de vous\n",
      "output = i m really proud of you <EOS>\n"
     ]
    }
   ],
   "source": [
    "#Testing different sentences\n",
    "evaluateAndShowAttention('il n est pas aussi grand que son pere')\n",
    "\n",
    "evaluateAndShowAttention('je suis trop fatigue pour conduire')\n",
    "\n",
    "evaluateAndShowAttention('je suis desole si c est une question idiote')\n",
    "\n",
    "evaluateAndShowAttention('je suis reellement fiere de vous')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
